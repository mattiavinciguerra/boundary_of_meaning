{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0866e23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import CLIPProcessor\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8036e95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTextDataset(Dataset):\n",
    "    def __init__(self, jsonl_path, processor):\n",
    "        with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = [json.loads(line) for line in f]\n",
    "        random.seed(42)\n",
    "        random.shuffle(data)\n",
    "        self.data = data\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        image = Image.open(item[\"image_path\"]).convert(\"RGB\")\n",
    "        caption = item[\"caption\"]\n",
    "        label = int(item[\"label\"])\n",
    "        inputs = self.processor(text=caption, images=image, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
    "        return inputs, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdf8dc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_embeds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Extracting embeddings\"):\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                pixel_values=inputs[\"pixel_values\"]\n",
    "            )\n",
    "\n",
    "            # Mean on text and image embeddings\n",
    "            combined_embeds = (outputs.text_embeds + outputs.image_embeds) / 2\n",
    "\n",
    "            all_embeds.append(combined_embeds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    all_embeds = np.vstack(all_embeds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    np.savez(f\"clip_embeddings.npz\", embeddings=all_embeds, labels=all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3980eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting embeddings: 100%|██████████| 377/377 [07:26<00:00,  1.18s/it]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset_path = \"combined_dataset.jsonl\"\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\", use_fast=False)\n",
    "dataset = ImageTextDataset(dataset_path, processor)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "\n",
    "extract_embeddings(model, dataloader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
